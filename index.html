<!DOCTYPE html>
<html>
    <title>AWE</title>

    <meta charset="UTF-8">
    <meta property="og:title" content="Hi Robot">
    <meta property="og:description" content="Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models">
    <meta property="og:url" content="">
    <meta property="og:image" content="">
    <meta property="og:type" content="website">
    <meta name="viewport" content="width=device-width, initial-scale=1 minimum-scale=1.0">

    <link rel="icon" type="image/png" href="">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto:100, 100i,300,400,500,700,900" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

    <!-- Showdown -->
    <script src=" https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
    <script src="js/figure-extension.js"></script>

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- WAVE -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>

    <!-- Slick -->
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.css"/>
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick-theme.css"/>
    <script type="text/javascript" src="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>

    <link rel="stylesheet" href="theme.css">

    <script>
        const classMap = {
            ul: 'browser-default'
        }

        const bindings = Object.keys(classMap)
        .map(key => ({
            type: 'output',
            regex: new RegExp(`<${key}(.*)>`, 'g'),
            replace: `<${key} class="${classMap[key]}" $1>`
        }));

        const converter = new showdown.Converter({
            extensions: [bindings, 'figure']
        });
        converter.setOption('parseImgDimensions', true);
        converter.setOption('tables', true);
        converter.setFlavor('github');

        $("#markdown-body").ready(() => {
            $.get( "content.md", (data) => {
                const content_html = converter.makeHtml(data);
                $("#markdown-body").html(content_html);
            });
        });

    </script>

    <body>
        <!-- Header -->
        <!-- Wide screen -->
        <header class="hd-container w3-container hide-narrow content-center">
            <div class="w3-cell-row" style="width: 90%; margin: auto; max-width: 1600px; margin-top: 80px; margin-bottom: 40px">
                <div class="w3-container w3-cell w3-cell-middle">
                    <div class="title">Hi Robot: Open-Ended Instruction Following with Hierarchical</div>
                    <div class="title">Vision-Language-Action Models</div>
                    <!-- Author -->
                <div class="w3-row-padding">
                    <div class="authorship-container">
                        <ul class="horizontal-list">
                            <li><i class="far fa-user"></i> Anonymous Authors</li>
                        </ul>
                        <!-- <ul class="horizontal-list">
                            <li><i class="fas fa-university"></i> Stanford University</li>
                            <li><i class="fas fa-university"></i> equal contribution<sup>*</sup></li>
                        </ul> -->
                    </div>
                    </div>
                    <div class="excerpt w3-padding-16" style="width: 80%; max-width: 700px; margin: auto;">
                        Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason
                        about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback
                        during task execution. Intricate instructions (e.g., "Could you make me a vegetarian sandwich?" or "I don't like that
                        one") require not just the ability to physically perform the individual steps, but the ability to situate complex
                        commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a
                        hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step
                        to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following
                        methods that can fulfill simple commands ("pick up the cup"), our system can reason through complex prompts and
                        incorporate situated feedback during task execution ("that's not trash").
                        We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots,
                        demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping.
                    </div>
                </div>
            </div>
        </header>

        <!-- Narrow screen -->
        <header class="hd-container w3-container hide-wide">
            <div class="w3-row-padding w3-center w3-padding-24">
                <span class="title">Hi Robot: Open-Ended Instruction Following with Hierarchical</span>
                <span class="title">Vision-Language-Action Models</span>
            </div>
            <div class="w3-row-padding">
                <!-- Author -->
                <div class="authorship-container">
                    <ul class="horizontal-list">
                        <li><i class="far fa-user"></i> Anonymous Authors</li>
                    </ul>
                    <!-- <ul class="horizontal-list">
                        <li><i class="fas fa-university"></i> Stanford University</li>
                </div>

            </div>
            <div class="w3-row-padding"><hr></div>
                <div class="w3-row-padding w3-padding-16">
                    <div class="excerpt">
                        While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of
                        <i>compounding errors</i> continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the
                        horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is
                        underspecified, and requires additional human supervision. Can we generate waypoints automatically without any
                        additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the
                        endpoints can be used as waypoints. We propose <i>Automatic Waypoint Extraction</i> (AWE) for imitation learning, a
                        preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can
                        approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find
                        that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on
                        real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10.
                    </div>
            </div>
        </header>

        <!-- Main Body -->
        <div class="main-body">
            <div class="w3-container">
                <div class="w3-content" style="max-width:1000px;">
                    <!-- Links -->
                    <div class="link-container">
                        <ul class="horizontal-list">
                            <!-- replace later -->
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-file-alt"></i> <a href="https://anoncorl175.github.io/awe-imitation-learning" target="_blank"> Paper </a></button></li>
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-code"></i> <a href="https://anoncorl175.github.io/awe-imitation-learning" target="_blank"> Code </a></button></li>
                        </ul>
                    </div>
                    <!-- Markdown Body -->
                    <div id="markdown-body"></div>
                </div>
            </div>
        </div>

        <!-- Footer -->
        <!-- <footer class="w3-center w3-light-grey w3-padding-32 w3-small">
            <p style="color: grey">
                The website template is borrowed from <a href="https://clvrai.github.io/skill-chaining/">here.</a>  <br/>
                We would like to thank Suneel Belkhale and Chen Wang for helpful discussions, and all members of the IRIS lab for
                constructive feedback. <br />
                &copy; Copyright 2023, IRIS Lab, Stanford University.
            </p>
        </footer> -->

    </body>
</html>
